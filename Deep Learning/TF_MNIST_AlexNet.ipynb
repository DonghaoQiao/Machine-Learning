{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    " \n",
    "batch_size=32\n",
    "num_batch=100\n",
    "keep_prob=0.5\n",
    " \n",
    " \n",
    "def print_architecture(t):\n",
    "    \"\"\"print the architecture information of the network,include name and size\"\"\"\n",
    "    print(t.op.name,\" \",t.get_shape().as_list())\n",
    " \n",
    " \n",
    "def inference(images):\n",
    "    \"\"\" 构建网络 ：5个conv+3个FC\"\"\"\n",
    "    parameters=[]  #储存参数\n",
    " \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        \"\"\"\n",
    "        images:227*227*3\n",
    "        kernel: 11*11 *64\n",
    "        stride:4*4\n",
    "        padding:name      \n",
    "        \n",
    "        #通过with tf.name_scope('conv1') as scope可以将scope内生成的Variable自动命名为conv1/xxx\n",
    "        便于区分不同卷积层的组建\n",
    "        \n",
    "        input: images[227*227*3]\n",
    "        middle: conv1[55*55*96]\n",
    "        output: pool1 [27*27*96]\n",
    "        \n",
    "        \"\"\"\n",
    "        kernel=tf.Variable(tf.truncated_normal([11,11,3,96],\n",
    "                           dtype=tf.float32,stddev=0.1),name=\"weights\")\n",
    "        conv=tf.nn.conv2d(images,kernel,[1,4,4,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0, shape=[96],  dtype=tf.float32),\n",
    "                           trainable=True,name=\"biases\")\n",
    "        bias=tf.nn.bias_add(conv,biases) # w*x+b\n",
    "        conv1=tf.nn.relu(bias,name=scope) # reLu\n",
    "        print_architecture(conv1)\n",
    "        parameters +=[kernel,biases]\n",
    " \n",
    "        #添加LRN层和max_pool层\n",
    "        \"\"\"\n",
    "        LRN会让前馈、反馈的速度大大降低（下降1/3），但最终效果不明显，所以只有ALEXNET用LRN，其他模型都放弃了\n",
    "        \"\"\"\n",
    "        lrn1=tf.nn.lrn(conv1,depth_radius=4,bias=1,alpha=0.001/9,beta=0.75,name=\"lrn1\")\n",
    "        pool1=tf.nn.max_pool(lrn1,ksize=[1,3,3,1],strides=[1,2,2,1],\n",
    "                             padding=\"VALID\",name=\"pool1\")\n",
    "        print_architecture(pool1)\n",
    " \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        \"\"\"\n",
    "        input: pool1[27*27*96]\n",
    "        middle: conv2[27*27*256]\n",
    "        output: pool2 [13*13*256]\n",
    " \n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([5, 5, 96, 256],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)  # w*x+b\n",
    "        conv2 = tf.nn.relu(bias, name=scope)  # reLu\n",
    "        parameters += [kernel, biases]\n",
    "        # 添加LRN层和max_pool层\n",
    "        \"\"\"\n",
    "        LRN会让前馈、反馈的速度大大降低（下降1/3），但最终效果不明显，所以只有ALEXNET用LRN，其他模型都放弃了\n",
    "        \"\"\"\n",
    "        lrn2 = tf.nn.lrn(conv2, depth_radius=4, bias=1, alpha=0.001 / 9, beta=0.75, name=\"lrn1\")\n",
    "        pool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding=\"VALID\", name=\"pool2\")\n",
    "        print_architecture(pool2)\n",
    " \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        \"\"\"\n",
    "        input: pool2[13*13*256]\n",
    "        output: conv3 [13*13*384]\n",
    " \n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)  # w*x+b\n",
    "        conv3 = tf.nn.relu(bias, name=scope)  # reLu\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(conv3)\n",
    " \n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        \"\"\"\n",
    "        input: conv3[13*13*384]\n",
    "        output: conv4 [13*13*384]\n",
    " \n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)  # w*x+b\n",
    "        conv4 = tf.nn.relu(bias, name=scope)  # reLu\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(conv4)\n",
    " \n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        \"\"\"\n",
    "        input: conv4[13*13*384]\n",
    "        output: conv5 [6*6*256]\n",
    " \n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)  # w*x+b\n",
    "        conv5 = tf.nn.relu(bias, name=scope)  # reLu\n",
    "        pool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                               padding=\"VALID\", name=\"pool5\")\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(pool5)\n",
    " \n",
    "    #全连接层6\n",
    "    with tf.name_scope('fc6') as scope:\n",
    "        \"\"\"\n",
    "        input:pool5 [6*6*256]\n",
    "        output:fc6 [4096]\n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([6*6*256,4096],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[4096], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        # 输入数据变换\n",
    "        flat = tf.reshape(pool5, [-1, 6*6*256] )  # 整形成m*n,列n为7*7*64\n",
    "        # 进行全连接操作\n",
    "        fc = tf.nn.relu(tf.matmul(flat, kernel) + biases,name='fc6')\n",
    "        # 防止过拟合  nn.dropout\n",
    "        fc6 = tf.nn.dropout(fc, keep_prob)\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(fc6)\n",
    " \n",
    "    # 全连接层7\n",
    "    with tf.name_scope('fc7') as scope:\n",
    "        \"\"\"\n",
    "        input:fc6 [4096]\n",
    "        output:fc7 [4096]\n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[4096], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        # 进行全连接操作\n",
    "        fc = tf.nn.relu(tf.matmul(fc6, kernel) + biases, name='fc7')\n",
    "        # 防止过拟合  nn.dropout\n",
    "        fc7 = tf.nn.dropout(fc, keep_prob)\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(fc7)\n",
    " \n",
    "    # 全连接层8\n",
    "    with tf.name_scope('fc8') as scope:\n",
    "        \"\"\"\n",
    "        input:fc7 [4096]\n",
    "        output:fc8 [1000]\n",
    "        \"\"\"\n",
    "        kernel = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                 dtype=tf.float32, stddev=0.1), name=\"weights\")\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[1000], dtype=tf.float32),\n",
    "                             trainable=True, name=\"biases\")\n",
    "        # 进行全连接操作\n",
    "        fc8 = tf.nn.xw_plus_b(fc7, kernel, biases, name='fc8')\n",
    "        parameters += [kernel, biases]\n",
    "        print_architecture(fc8)\n",
    " \n",
    "    return fc8,parameters\n",
    " \n",
    "def time_compute(session,target,info_string):\n",
    "    num_step_burn_in=10  #预热轮数，头几轮迭代有显存加载、cache命中等问题可以因此跳过\n",
    "    total_duration=0.0   #总时间\n",
    "    total_duration_squared=0.0\n",
    "    for i in range(num_batch+num_step_burn_in):\n",
    "        start_time=time.time()\n",
    "        _ = session.run(target)\n",
    "        duration= time.time() -start_time\n",
    "        if i>= num_step_burn_in:\n",
    "            if i%10==0: #每迭代10次显示一次duration\n",
    "                print(\"%s: step %d,duration=%.5f \"% (datetime.now(),i-num_step_burn_in,duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration *duration\n",
    "    time_mean=total_duration /num_batch\n",
    "    time_variance=total_duration_squared / num_batch - time_mean*time_mean\n",
    "    time_stddev=math.sqrt(time_variance)\n",
    "    #迭代完成，输出\n",
    "    print(\"%s: %s across %d steps,%.3f +/- %.3f sec per batch \"%\n",
    "              (datetime.now(),info_string,num_batch,time_mean,time_stddev))\n",
    " \n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        \"\"\"仅使用随机图片数据 测试前馈和反馈计算的耗时\"\"\"\n",
    "        image_size =224\n",
    "        images=tf.Variable(tf.random_normal([batch_size,image_size,image_size,3],\n",
    "                                     dtype=tf.float32,stddev=0.1 ) )\n",
    "        fc8,parameters=inference(images)\n",
    " \n",
    "        init=tf.global_variables_initializer()\n",
    "        sess=tf.Session()\n",
    "        sess.run(init)\n",
    " \n",
    "        \"\"\"\n",
    "        AlexNet forward 计算的测评\n",
    "        传入的target:fc8（即最后一层的输出）\n",
    "        优化目标：loss\n",
    "        使用tf.gradients求相对于loss的所有模型参数的梯度\n",
    "        \n",
    "        \n",
    "        AlexNet Backward 计算的测评\n",
    "        target:grad\n",
    "         \n",
    "        \"\"\"\n",
    "        time_compute(sess,target=fc8,info_string=\"Forward\")\n",
    " \n",
    "        obj=tf.nn.l2_loss(fc8)\n",
    "        grad=tf.gradients(obj,parameters)\n",
    "        time_compute(sess,grad,\"Forward-backward\")\n",
    " \n",
    " \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
