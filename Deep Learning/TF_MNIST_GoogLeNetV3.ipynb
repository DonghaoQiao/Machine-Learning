{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    " \n",
    "slim=tf.contrib.slim\n",
    "#产生截断的正态分布\n",
    "trunc_normal =lambda stddev:tf.truncated_normal_initializer(0.0,stddev)\n",
    "parameters =[] #储存参数\n",
    " \n",
    " \n",
    "#why？为什么要定义这个函数？\n",
    "#因为若事先定义好slim.conv2d各种默认参数，包括激活函数、标准化器，后面定义卷积层将会非常容易：\n",
    "# 1.代码整体美观\n",
    "# 2.网络设计的工作量会大大减轻\n",
    " \n",
    "def inception_v3_arg_scope(weight_decay=0.00004,\n",
    "                           stddev=0.1,\n",
    "                           batch_norm_var_collection='moving_vars'):\n",
    "    \"\"\"\n",
    "    #定义inception_v3_arg_scope(),\n",
    "    #用来生成网络中经常用到的函数的默认参数（卷积的激活函数、权重初始化方式、标准化器等）\n",
    "    :param weight_decay: 权值衰减系数\n",
    "    :param stddev: 标准差\n",
    "    :param batch_norm_var_collection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_norm_params={\n",
    "        'decay':0.9997, #衰减系数decay\n",
    "        'epsilon':0.001, #极小值\n",
    "        'updates_collections':tf.GraphKeys.UPDATE_OPS,\n",
    "        'variables_collections':{\n",
    "            'beta':None,\n",
    "            'gamma':None,\n",
    "            'moving_mean':[batch_norm_var_collection],\n",
    "            'moving_variance':[batch_norm_var_collection],\n",
    "        }\n",
    " \n",
    "    }\n",
    " \n",
    "    with slim.arg_scope([slim.conv2d,slim.fully_connected],\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        \"\"\"\n",
    "        slim.arg_scope()是一个非常有用的工具，可以给函数的参数自动赋予某些默认值\n",
    "        \n",
    "        例如：\n",
    "        slim.arg_scope([slim.conv2d,slim.fully_connected],weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        会对[slim.conv2d,slim.fully_connected]这两个函数的参数自动赋值，\n",
    "        将参数weights_regularizer的默认值设为slim.l2_regularizer(weight_decay)\n",
    "        \n",
    "        备注：使用了slim.arg_scope后就不需要每次重复设置参数，只需在修改时设置即可。\n",
    "        \"\"\"\n",
    "        # 设置默认值：对slim.conv2d函数的几个参数赋予默认值\n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d],\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=stddev), #权重初始化\n",
    "            activation_fn=tf.nn.relu,  #激励函数\n",
    "            normalizer_fn=slim.batch_norm,  #标准化器\n",
    "            normalizer_params=batch_norm_params ) as sc: #normalizer_params标准化器的参数\n",
    " \n",
    "            return sc  #返回定义好的scope\n",
    " \n",
    " \n",
    "def inception_V3_base(input,scope=None):\n",
    " \n",
    "    end_points= {}\n",
    "    # 第一部分--基础部分：卷积和池化交错\n",
    "    with tf.variable_scope(scope,'inception_V3',[input]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                            stride=1,padding='VALID'):\n",
    "            net1=slim.conv2d(input,32,[3,3],stride=2,scope='conv2d_1a_3x3')\n",
    "            net2 = slim.conv2d(net1, 32, [3, 3],scope='conv2d_2a_3x3')\n",
    "            net3 = slim.conv2d(net2, 64, [3, 3], padding='SAME',\n",
    "                               scope='conv2d_2b_3x3')\n",
    "            net4=slim.max_pool2d(net3,[3,3],stride=2,scope='maxPool_3a_3x3')\n",
    "            net5 = slim.conv2d(net4, 80, [1, 1], scope='conv2d_4a_3x3')\n",
    "            net6 = slim.conv2d(net5, 192, [3, 3], padding='SAME',\n",
    "                               scope='conv2d_4b_3x3')\n",
    "            net = slim.max_pool2d(net6, [3, 3], stride=2, scope='maxPool_5a_3x3')\n",
    " \n",
    "    #第二部分--Inception模块组：inception_1\\inception_2\\inception_2\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                            stride=1,padding='SAME'):\n",
    "#inception_1：第一个模块组（共含3个inception_module）\n",
    "        #inception_1_m1: 第一组的1号module\n",
    "        with tf.variable_scope('inception_1_m1'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0=slim.conv2d(net,64,[1,1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 48, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 64, [5, 5],\n",
    "                                        scope='conv2d_1b_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 64, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 96, [3, 3],\n",
    "                                        scope='conv2d_2b_3x3')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 96, [3, 3],\n",
    "                                        scope='conv2d_2c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 32, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    "            #使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net=tf.concat([branch_0,branch1_2,branch2_3,branch3_2],3)\n",
    " \n",
    "        # inception_1_m2: 第一组的 2号module\n",
    "        with tf.variable_scope('inception_1_m2'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 48, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 64, [5, 5],\n",
    "                                            scope='conv2d_1b_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 64, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 96, [3, 3],\n",
    "                                            scope='conv2d_2b_3x3')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 96, [3, 3],\n",
    "                                            scope='conv2d_2c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 64, [1, 1],\n",
    "                                            scope='conv2d_3b_1x1')\n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_2, branch2_3, branch3_2], 3)\n",
    " \n",
    "        # inception_1_m2: 第一组的 3号module\n",
    "        with tf.variable_scope('inception_1_m3'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 48, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 64, [5, 5],\n",
    "                                            scope='conv2d_1b_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 64, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 96, [3, 3],\n",
    "                                            scope='conv2d_2b_3x3')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 96, [3, 3],\n",
    "                                            scope='conv2d_2c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 64, [1, 1],\n",
    "                                            scope='conv2d_3b_1x1')\n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_2, branch2_3, branch3_2], 3)\n",
    " \n",
    "#inception_2：第2个模块组（共含5个inception_module）\n",
    "        # inception_2_m1: 第2组的 1号module\n",
    "        with tf.variable_scope('inception_2_m1'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 384, [3, 3],stride=2,\n",
    "                                       padding='VALID',scope='conv2d_0a_3x3')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 64, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 96, [3, 3],\n",
    "                                            scope='conv2d_1b_3x3')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 96, [3, 3],\n",
    "                                        stride=2,\n",
    "                                        padding='VALID',\n",
    "                                        scope='conv2d_1c_3x3')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.max_pool2d(net, [3, 3],\n",
    "                                        stride=2,\n",
    "                                        padding='VALID',\n",
    "                                        scope='maxPool_2a_3x3')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_3, branch2_1], 3)\n",
    " \n",
    "        # inception_2_m2: 第2组的 2号module\n",
    "        with tf.variable_scope('inception_2_m2'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 128, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 128, [1, 7],\n",
    "                                            scope='conv2d_1b_1x7')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 128, [7, 1],\n",
    "                                        scope='conv2d_1c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 128, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 128, [7, 1],\n",
    "                                        scope='conv2d_2b_7x1')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 128, [1, 7],\n",
    "                                        scope='conv2d_2c_1x7')\n",
    "                branch2_4 = slim.conv2d(branch2_3, 128, [7, 1],\n",
    "                                        scope='conv2d_2d_7x1')\n",
    "                branch2_5 = slim.conv2d(branch2_4, 128, [1, 7],\n",
    "                                        scope='conv2d_2e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], 3)\n",
    " \n",
    " \n",
    "# inception_2_m3: 第2组的 3号module\n",
    "        with tf.variable_scope('inception_2_m3'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 160, [1, 7],\n",
    "                                            scope='conv2d_1b_1x7')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 192, [7, 1],\n",
    "                                        scope='conv2d_1c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 160, [7, 1],\n",
    "                                        scope='conv2d_2b_7x1')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 160, [1, 7],\n",
    "                                        scope='conv2d_2c_1x7')\n",
    "                branch2_4 = slim.conv2d(branch2_3, 160, [7, 1],\n",
    "                                        scope='conv2d_2d_7x1')\n",
    "                branch2_5 = slim.conv2d(branch2_4, 192, [1, 7],\n",
    "                                        scope='conv2d_2e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], 3)\n",
    " \n",
    "# inception_2_m4: 第2组的 4号module\n",
    "        with tf.variable_scope('inception_2_m4'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 160, [1, 7],\n",
    "                                            scope='conv2d_1b_1x7')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 192, [7, 1],\n",
    "                                        scope='conv2d_1c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 160, [7, 1],\n",
    "                                        scope='conv2d_2b_7x1')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 160, [1, 7],\n",
    "                                        scope='conv2d_2c_1x7')\n",
    "                branch2_4 = slim.conv2d(branch2_3, 160, [7, 1],\n",
    "                                        scope='conv2d_2d_7x1')\n",
    "                branch2_5 = slim.conv2d(branch2_4, 192, [1, 7],\n",
    "                                        scope='conv2d_2e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], 3)\n",
    " \n",
    "# inception_2_m5: 第2组的 5号module\n",
    "        with tf.variable_scope('inception_2_m5'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 160, [1, 7],\n",
    "                                            scope='conv2d_1b_1x7')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 192, [7, 1],\n",
    "                                        scope='conv2d_1c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 160, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 160, [7, 1],\n",
    "                                        scope='conv2d_2b_7x1')\n",
    "                branch2_3 = slim.conv2d(branch2_2, 160, [1, 7],\n",
    "                                        scope='conv2d_2c_1x7')\n",
    "                branch2_4 = slim.conv2d(branch2_3, 160, [7, 1],\n",
    "                                        scope='conv2d_2d_7x1')\n",
    "                branch2_5 = slim.conv2d(branch2_4, 192, [1, 7],\n",
    "                                        scope='conv2d_2e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_3, branch2_5,branch3_2], 3)\n",
    "        #将inception_2_m5存储到end_points中，作为Auxiliary Classifier辅助模型的分类\n",
    "        end_points['inception_2_m5']=net\n",
    " \n",
    "# 第3组\n",
    "# inception_3_m1: 第3组的 1号module\n",
    "        with tf.variable_scope('inception_3_m1'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1],scope='conv2d_0a_1x1')\n",
    "                branch_0 = slim.conv2d(branch_0,320, [3, 3],\n",
    "                                       stride=2,\n",
    "                                       padding='VALID',\n",
    "                                       scope='conv2d_0b_3x3')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 192, [1, 1], scope='conv2d_1a_1x1')\n",
    "                branch1_2 = slim.conv2d(branch1_1, 192, [1, 7],\n",
    "                                            scope='conv2d_1b_1x7')\n",
    "                branch1_3 = slim.conv2d(branch1_2, 192, [7, 1],\n",
    "                                        scope='conv2d_1c_7x1')\n",
    "                branch1_4 = slim.conv2d(branch1_3, 192, [3, 3],\n",
    "                                        stride=2,\n",
    "                                        padding='VALID',\n",
    "                                        scope='conv2d_1c_3x3')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.max_pool2d(net, [3, 3],\n",
    "                                            stride=2,\n",
    "                                            padding='VALID',\n",
    "                                            scope='maxPool_3a_3x3')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_4, branch2_1], 3)\n",
    " \n",
    "# inception_3_m2: 第3组的 2号module\n",
    "        with tf.variable_scope('inception_3_m2'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 384, [1, 1], scope='conv2d_1a_1x1')\n",
    "#特殊\n",
    "                branch1_2 = tf.concat([\n",
    "                    slim.conv2d(branch1_1, 384, [1, 3], scope='conv2d_1a_1x3'),\n",
    "                    slim.conv2d(branch1_1, 384, [3, 1], scope='conv2d_1a_3x1')\n",
    "                    ], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 488, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 384, [3, 3],\n",
    "                                        scope='conv2d_2b_3x3')\n",
    "                branch2_3 = tf.concat([\n",
    "                    slim.conv2d(branch2_2, 384, [1, 3], scope='conv2d_1a_1x3'),\n",
    "                    slim.conv2d(branch2_2, 384, [3, 1], scope='conv2d_1a_3x1')\n",
    "                    ], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_2, branch2_3,branch3_2], 3)\n",
    " \n",
    "# inception_3_m3: 第3组的 3号module\n",
    "        with tf.variable_scope('inception_3_m3'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1],scope='conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch1_1 = slim.conv2d(net, 384, [1, 1], scope='conv2d_1a_1x1')\n",
    "#特殊\n",
    "                branch1_2 = tf.concat([\n",
    "                    slim.conv2d(branch1_1, 384, [1, 3], scope='conv2d_1a_1x3'),\n",
    "                    slim.conv2d(branch1_1, 384, [3, 1], scope='conv2d_1a_3x1')\n",
    "                    ], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch2_1 = slim.conv2d(net, 488, [1, 1], scope='conv2d_2a_1x1')\n",
    "                branch2_2 = slim.conv2d(branch2_1, 384, [3, 3],\n",
    "                                        scope='conv2d_2b_3x3')\n",
    "                branch2_3 = tf.concat([\n",
    "                    slim.conv2d(branch2_2, 384, [1, 3], scope='conv2d_1a_1x3'),\n",
    "                    slim.conv2d(branch2_2, 384, [3, 1], scope='conv2d_1a_3x1')\n",
    "                    ], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch3_1 = slim.avg_pool2d(net, [3, 3], scope='avgPool_3a_3x3')\n",
    "                branch3_2 = slim.conv2d(branch3_1, 192, [1, 1],\n",
    "                                        scope='conv2d_3b_1x1')\n",
    " \n",
    "            # 使用concat将4个分支的输出合并到一起（在第三个维度合并，即输出通道上合并）\n",
    "            net = tf.concat([branch_0, branch1_2, branch2_3,branch3_2], 3)\n",
    " \n",
    "        return net,end_points\n",
    "##############################  卷积部分完成  ########################################\n",
    " \n",
    " \n",
    "#第三部分：全局平均池化、softmax、Auxiliary Logits\n",
    "def inception_v3(input,\n",
    "                 num_classes=1000,\n",
    "                 is_training=True,\n",
    "                 dropout_keep_prob=0.8,\n",
    "                 prediction_fn=slim.softmax,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='inceptionV3'):\n",
    "    with tf.variable_scope(scope,'inceptionV3',[input,num_classes],\n",
    "                       reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm,slim.dropout],\n",
    "                            is_training=is_training):\n",
    "             net,end_points=inception_V3_base(input,scope=scope)\n",
    " \n",
    "            #Auxiliary  Logits\n",
    "             with slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],\n",
    "                            stride=1,padding='SAME'):\n",
    "                aux_logits=end_points['inception_2_m5']\n",
    "                with tf.variable_scope('Auxiliary_Logits'):\n",
    "                    aux_logits=slim.avg_pool2d(\n",
    "                        aux_logits,[5,5],stride=3,padding='VALID',\n",
    "                        scope='AvgPool_1a_5x5' )\n",
    "                    aux_logits=slim.conv2d(aux_logits,128,[1,1],\n",
    "                                       scope='conv2d_1b_1x1')\n",
    "                    aux_logits=slim.conv2d(aux_logits,768,[5,5],\n",
    "                                       weights_initializer=trunc_normal(0.01),\n",
    "                                       padding='VALID',\n",
    "                                       scope='conv2d_2a_5x5')\n",
    "                    aux_logits = slim.conv2d(aux_logits, num_classes, [1, 1],\n",
    "                                         activation_fn=None,\n",
    "                                         normalizer_fn=None,\n",
    "                                         weights_initializer=trunc_normal(0.001),\n",
    "                                         scope='conv2d_2b_1x1')\n",
    "                    if spatial_squeeze:\n",
    "                        aux_logits =tf.squeeze(aux_logits,[1,2],name='SpatialSqueeze')\n",
    "                    end_points['Auxiliary_Logits']=aux_logits\n",
    " \n",
    "             with tf.variable_scope('Logits'):\n",
    "                 net=slim.avg_pool2d(net,[8,8],padding='VALID',\n",
    "                                scope='avgPool_1a_8x8')\n",
    "                 net=slim.dropout(net,keep_prob=dropout_keep_prob,\n",
    "                             scope='dropout_1b')\n",
    "                 end_points['PreLogits']=net\n",
    "                 logits=slim.conv2d(net,num_classes,[1,1],activation_fn=None,\n",
    "                               normalizer_fn=None,\n",
    "                               scope='conv2d_1c_1x1')\n",
    "                 if spatial_squeeze:\n",
    "                     logits=tf.squeeze(logits,[1,2],name='SpatialSqueeze')\n",
    "             end_points['Logits']=logits\n",
    "             end_points['Predictions']=prediction_fn(logits,scope='Predictions')\n",
    " \n",
    "    return logits,end_points\n",
    " \n",
    "########################### 构建完成\n",
    " \n",
    " \n",
    "def time_compute(session, target, info_string):\n",
    "    num_batch = 100 #100\n",
    "    num_step_burn_in = 10  # 预热轮数，头几轮迭代有显存加载、cache命中等问题可以因此跳过\n",
    "    total_duration = 0.0  # 总时间\n",
    "    total_duration_squared = 0.0\n",
    "    for i in range(num_batch + num_step_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target )\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_step_burn_in:\n",
    "            if i % 10 == 0:  # 每迭代10次显示一次duration\n",
    "                print(\"%s: step %d,duration=%.5f \" % (datetime.now(), i - num_step_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    time_mean = total_duration / num_batch\n",
    "    time_variance = total_duration_squared / num_batch - time_mean * time_mean\n",
    "    time_stddev = math.sqrt(time_variance)\n",
    "    # 迭代完成，输出\n",
    "    print(\"%s: %s across %d steps,%.3f +/- %.3f sec per batch \" %\n",
    "          (datetime.now(), info_string, num_batch, time_mean, time_stddev))\n",
    " \n",
    " \n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        batch_size=32\n",
    "        height,weight=299,299\n",
    "        input=tf.random_uniform( (batch_size,height,weight,3) )\n",
    "        with slim.arg_scope(inception_v3_arg_scope()):\n",
    "            logits,end_points=inception_v3(input,is_training=False)\n",
    " \n",
    "        init=tf.global_variables_initializer()\n",
    "        sess=tf.Session()\n",
    "        # 将网络结构图写到文件中\n",
    "        writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "        sess.run(init)\n",
    "        num_batches=100\n",
    "        time_compute(sess,logits,'Forward')\n",
    " \n",
    " \n",
    " \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
