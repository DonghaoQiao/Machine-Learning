{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    " \n",
    "def conv_op(input,kh,kw,n_out,dh,dw,parameters,name):\n",
    "    \"\"\"\n",
    "    定义卷积层的操作\n",
    "    :param input: 输入的tensor\n",
    "    :param kh:卷积核的高\n",
    "    :param kw:卷积核的宽\n",
    "    :param n_out:输出通道数（即卷积核的数量）\n",
    "    :param dh:步长的高\n",
    "    :param dw:步长的宽\n",
    "    :param parameters:参数列表\n",
    "    :param name:层的名字\n",
    "    :return:返回卷积层的结果\n",
    "    \"\"\"\n",
    " \n",
    "    n_in = input.get_shape()[-1].value  #通道数\n",
    " \n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel =tf.get_variable(scope+'w',\n",
    "                                shape=[kh,kw,n_in,n_out],dtype=tf.float32,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        conv=tf.nn.conv2d(input,kernel,[1,dh,dw,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[n_out],dtype=tf.float32),\n",
    "                           trainable=True,name='b')\n",
    "        z=tf.nn.bias_add(conv,biases) # wx+b\n",
    "        activation =tf.nn.relu(z,name=scope)\n",
    "        parameters +=[kernel,biases]\n",
    "        return activation\n",
    " \n",
    "def fc_op(input,n_out,parameters,name):\n",
    "    \"\"\"\n",
    "    定义全连接层操作\n",
    "    注意：卷积层的结果要做扁平化才能和fc层相连接\n",
    "    :param input: 输入的tensor\n",
    "    :param n_out: 输出通道数（即神经元的数量）\n",
    "    :param parameters: 参数列表\n",
    "    :param name: 层的名字\n",
    "    :return: 返回全连接层的结果\n",
    "    \"\"\"\n",
    " \n",
    "    n_in=input.get_shape()[-1].value\n",
    " \n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel =tf.get_variable(scope+'w',\n",
    "                                shape=[n_in,n_out],dtype=tf.float32,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer() )\n",
    " \n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32),\n",
    "                             trainable=True, name='b')\n",
    "        activation=tf.nn.relu(tf.matmul(input,kernel)+biases, name=scope)\n",
    "        parameters +=[kernel,biases]\n",
    "        return activation\n",
    " \n",
    " \n",
    "def maxPool_op(input,kh,kw,dh,dw,name):\n",
    "    return tf.nn.max_pool(input,ksize=[1,kh,kw,1],strides=[1,dh,dw,1],\n",
    "                          padding='SAME',name=name)\n",
    " \n",
    " \n",
    " \n",
    "def vggNet(input,keep_prob):\n",
    "    parameters =[]\n",
    " \n",
    "    #conv1段\n",
    "    conv1_1 =conv_op(input,kh=3,kw=3,n_out=64,dh=1,dw=1,\n",
    "                    parameters=parameters,name='conv1_1')\n",
    "    conv1_2 =conv_op(conv1_1, kh=3, kw=3, n_out=64, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv1_2')\n",
    "    pool1 =maxPool_op(conv1_2,kh=2,kw=2,dh=2,dw=2,name='pool1')\n",
    " \n",
    "    # conv2段\n",
    "    conv2_1 = conv_op(pool1, kh=3, kw=3, n_out=128, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv2_1')\n",
    "    conv2_2 = conv_op(conv2_1, kh=3, kw=3, n_out=128, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv2_2')\n",
    "    pool2 = maxPool_op(conv2_2, kh=2, kw=2, dh=2, dw=2, name='pool2')\n",
    " \n",
    "    # conv3段\n",
    "    conv3_1 = conv_op(pool2, kh=3, kw=3, n_out=256, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv3_1')\n",
    "    conv3_2 = conv_op(conv3_1, kh=3, kw=3, n_out=256, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv3_2')\n",
    "    conv3_3 = conv_op(conv3_2, kh=3, kw=3, n_out=256, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv3_3')\n",
    "    pool3 = maxPool_op(conv3_3, kh=2, kw=2, dh=2, dw=2, name='pool3')\n",
    " \n",
    " \n",
    "    # conv4段\n",
    "    conv4_1 = conv_op(pool3, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv4_1')\n",
    "    conv4_2 = conv_op(conv4_1, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv4_2')\n",
    "    conv4_3 = conv_op(conv4_2, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv4_3')\n",
    "    pool4 = maxPool_op(conv4_3, kh=2, kw=2, dh=2, dw=2, name='pool4')\n",
    " \n",
    "    # conv5段\n",
    "    conv5_1 = conv_op(pool4, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv5_1')\n",
    "    conv5_2 = conv_op(conv5_1, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv5_2')\n",
    "    conv5_3 = conv_op(conv5_2, kh=3, kw=3, n_out=512, dh=1, dw=1,\n",
    "                      parameters=parameters, name='conv5_3')\n",
    "    pool5 = maxPool_op(conv5_3, kh=2, kw=2, dh=2, dw=2, name='pool5')\n",
    " \n",
    "    #将最后一个卷积层的结果扁平化:每个样本占一行\n",
    "    conv_shape=pool5.get_shape()\n",
    "    col=conv_shape[1].value *conv_shape[2].value * conv_shape[3].value\n",
    "    flat=tf.reshape(pool5, [-1,col],name='flat')\n",
    " \n",
    "    # fc6段\n",
    "    fc6 =fc_op(input=flat,n_out=4096,parameters=parameters,name='fc6')\n",
    "    fc6_dropout =tf.nn.dropout(fc6,keep_prob,name='fc6_drop')\n",
    " \n",
    "    # fc7段\n",
    "    fc7 = fc_op(input=fc6_dropout, n_out=4096, parameters=parameters, name='fc7')\n",
    "    fc7_dropout = tf.nn.dropout(fc7, keep_prob, name='fc7_drop')\n",
    " \n",
    "    # fc8段：最后一个全连接层，使用softmax进行处理得到分类输出概率\n",
    "    fc8=fc_op(input=fc7_dropout,n_out=1000,parameters=parameters,name='fc8')\n",
    "    softmax =tf.nn.softmax(fc8)\n",
    "    predictions =tf.arg_max(softmax,1)\n",
    "    return predictions,softmax,fc8,parameters\n",
    " \n",
    " \n",
    "def time_compute(session, target, feed,info_string):\n",
    "    num_batch = 100 #100\n",
    "    num_step_burn_in = 10  # 预热轮数，头几轮迭代有显存加载、cache命中等问题可以因此跳过\n",
    "    total_duration = 0.0  # 总时间\n",
    "    total_duration_squared = 0.0\n",
    "    for i in range(num_batch + num_step_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target,feed_dict=feed )\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_step_burn_in:\n",
    "            if i % 10 == 0:  # 每迭代10次显示一次duration\n",
    "                print(\"%s: step %d,duration=%.5f \" % (datetime.now(), i - num_step_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    time_mean = total_duration / num_batch\n",
    "    time_variance = total_duration_squared / num_batch - time_mean * time_mean\n",
    "    time_stddev = math.sqrt(time_variance)\n",
    "    # 迭代完成，输出\n",
    "    print(\"%s: %s across %d steps,%.3f +/- %.3f sec per batch \" %\n",
    "          (datetime.now(), info_string, num_batch, time_mean, time_stddev))\n",
    " \n",
    " \n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        \"\"\"仅使用随机图片数据 测试前馈和反馈计算的耗时\"\"\"\n",
    "        image_size = 224\n",
    "        batch_size = 2   #32\n",
    " \n",
    "        images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3],\n",
    "                                              dtype=tf.float32, stddev=0.1))\n",
    " \n",
    "        keep_prob=tf.placeholder(tf.float32)\n",
    "        predictions,softmax,fc8, parameters = vggNet(images,keep_prob)\n",
    " \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    " \n",
    "        \"\"\"\n",
    "        AlexNet forward 计算的测评\n",
    "        传入的target:fc8（即最后一层的输出）\n",
    "        优化目标：loss\n",
    "        使用tf.gradients求相对于loss的所有模型参数的梯度\n",
    " \n",
    " \n",
    "        AlexNet Backward 计算的测评\n",
    "        target:grad\n",
    " \n",
    "        \"\"\"\n",
    "        time_compute(sess, target=fc8, feed={keep_prob:1.0},info_string=\"Forward\")\n",
    " \n",
    "        obj = tf.nn.l2_loss(fc8)\n",
    "        grad = tf.gradients(obj, parameters)\n",
    "        time_compute(sess, grad, feed={keep_prob:0.5},info_string=\"Forward-backward\")\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
